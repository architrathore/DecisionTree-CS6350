{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of decision tree for Machine Learning - CS6350\n",
    "Author: Archit Rathore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataRecord:\n",
    "    '''Class to store individual data points read from train and test files'''\n",
    "\n",
    "    def __init__(self, label, name):\n",
    "        self.label = 1 if label == '+' else 0\n",
    "        self.name = ' '.join(name).lower()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.label) + ' ' + self.name\n",
    "    \n",
    "class DecisionNode:\n",
    "    '''Represent a node in the decision tree'''\n",
    "    def __init__(self, attr_index):\n",
    "        self.attr_index = attr_index    # index of attribute that is tested in the \n",
    "                                        # feature array, -1 for leaf nodes\n",
    "        self.prediction = None          # the prediction from this node, None for all non-leaf nodes\n",
    "        self.branches = {}              # dictionary holding subtrees of the tree, empty for leaf nodes\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.attr_index == -1:\n",
    "            return '[\"{0}\"]'.format(self.prediction)\n",
    "        else:\n",
    "            return '[{}]'.format(str(self.attr_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    '''Read train or test file and return a list DataRecord objects'''\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip().split()\n",
    "            label, name = line[0], line[1:]\n",
    "            data.append(DataRecord(label, name))\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_features(dataset, func_list):\n",
    "    '''Apply a sequence of functions to the dataset to get a feature array\n",
    "    for each data point'''\n",
    "    n, d = len(dataset), len(func_list)\n",
    "    X = np.empty((n, d), dtype=int)\n",
    "    for idx, datapoint in enumerate(dataset):\n",
    "        X[idx] = np.array([f(datapoint.name) for f in func_list])\n",
    "    return X\n",
    "\n",
    "\n",
    "def check_same_label(arr):\n",
    "    '''Check if all elements in arr are identical and return it'''\n",
    "    return len(set(arr)) == 1\n",
    "\n",
    "\n",
    "def get_entropy(labels):\n",
    "    prob_pos, prob_neg = sum(labels == 1)/len(labels), sum(labels == 0)/len(labels)\n",
    "    if prob_pos == 0 or prob_neg == 0:\n",
    "        return 0\n",
    "    entropy = prob_pos * np.log2(prob_pos) + prob_neg * np.log2(prob_neg)\n",
    "    return -1.0 * entropy\n",
    "    \n",
    "    \n",
    "def find_best_split(S, attributes, labels):\n",
    "    # Return the first attribute for now\n",
    "    # return list(attributes)[0]\n",
    "    '''Implements information gain based on reduction in entropy\n",
    "    for feature selection'''\n",
    "    \n",
    "    if len(attributes) == 1:\n",
    "        return next(iter(attributes))\n",
    "    max_info_gain = 0\n",
    "    best_split_attribute = None\n",
    "    root_node_entropy = get_entropy(labels)\n",
    "    for attribute in attributes:\n",
    "        attribute_value_set = set(S[:, attribute])\n",
    "        weighted_entropy = 0\n",
    "        for value in attribute_value_set:\n",
    "            value_indices = S[:, attribute] == value\n",
    "            value_labels = labels[value_indices]\n",
    "            weighted_entropy += len(value_labels) * get_entropy(value_labels)\n",
    "        info_gain = root_node_entropy - weighted_entropy/len(S)\n",
    "        if info_gain >= max_info_gain:\n",
    "            best_split_attribute = attribute\n",
    "            max_info_gain = info_gain\n",
    "    return best_split_attribute\n",
    "\n",
    "\n",
    "def ID3(S, attributes, labels, depth=0, max_depth=100):\n",
    "    '''ID3 algorithm for building a decision tree'''\n",
    "\n",
    "    # If all examples have the same label\n",
    "    if check_same_label(labels) or len(attributes) == 0 or depth >= max_depth:\n",
    "        # Return a single node tree with the label\n",
    "        leaf_node = DecisionNode(-1)\n",
    "        leaf_node.prediction = labels[0]\n",
    "        return leaf_node\n",
    "\n",
    "    else:\n",
    "        # Create a root node for tree\n",
    "        root_node = DecisionNode(-1)\n",
    "\n",
    "        # print(\"Attributes\", attributes)\n",
    "        # Find attribute A that best classifies the dataset S\n",
    "        splitting_attr = find_best_split(S, attributes, labels)\n",
    "        root_node.attr_index = splitting_attr\n",
    "        \n",
    "        # For each value v that the A can take:\n",
    "        for split_val in set(S[:, splitting_attr]):\n",
    "            # Find subset of examples S_v with A = v\n",
    "\n",
    "            # Get indices with splitting_att = split_val\n",
    "            indices = S[:, splitting_attr] == split_val\n",
    "            # indices is a boolean indicator array\n",
    "\n",
    "            # If S_v is empty\n",
    "            if sum(indices) == 0:\n",
    "                # Find the most common label in S\n",
    "                most_common_label = Counter(labels).most_common(1)\n",
    "                # Create a leaf node with this label\n",
    "                leaf_node = DecisionNode(-1)\n",
    "                leaf_node.prediction = most_common_label\n",
    "                # Add the leaf node to this branch of node\n",
    "                root_node.branches[split_val] = leaf_node\n",
    "\n",
    "            else:\n",
    "                # Add subtree ID3(S_v, Attributes - {A}, Label_v)\n",
    "                S_v = S[indices]\n",
    "                label_v = labels[indices]\n",
    "                remaining_attr = attributes - {splitting_attr}\n",
    "                root_node.branches[split_val] = ID3(S_v, remaining_attr, label_v, \n",
    "                                                    depth=depth + 1, max_depth=max_depth)\n",
    "\n",
    "        return root_node\n",
    "\n",
    "\n",
    "def print_tree(decision_tree):\n",
    "    '''Do a level order traversal to print the decision tree'''\n",
    "    # Create two queues, one holds the current depth nodes\n",
    "    # and the other holds the nodes of the next level\n",
    "    curr_level, next_level = [], []\n",
    "    \n",
    "    # Append the root to current level's queue\n",
    "    curr_level.append(decision_tree)\n",
    "    \n",
    "    # While there are still nodes to visit\n",
    "    while (len(curr_level) > 0):\n",
    "        # Get the first node in queue\n",
    "        curr_node = curr_level.pop(0)\n",
    "        \n",
    "        # Print it's data\n",
    "        print(curr_node, end=' ')\n",
    "        \n",
    "        # For each child of the current node\n",
    "        for branch in curr_node.branches:\n",
    "            # Add all children to the next level's queue\n",
    "            next_level.append(curr_node.branches[branch])\n",
    "            \n",
    "        # If the current level's queue is empty\n",
    "        if len(curr_level) == 0:\n",
    "            print()\n",
    "            # Swap current and next level queues\n",
    "            curr_level, next_level = next_level, curr_level\n",
    "\n",
    "\n",
    "def predict(decision_tree, x):\n",
    "    '''Given a decision tree and input, find the prediction'''\n",
    "\n",
    "    # print('Attrib: ', decision_tree.attr_index)\n",
    "    # If the given node is leaf\n",
    "    if decision_tree.attr_index == -1:\n",
    "        return decision_tree.prediction\n",
    "    \n",
    "    # Recursively call prediction based on current node's splitting attribute\n",
    "    else:\n",
    "        decision_attr_val = x[decision_tree.attr_index]\n",
    "        # print('Dec_attr_index:', decision_tree.attr_index)\n",
    "        # print('Dec attrib value: ', decision_attr_val)\n",
    "        # print('Branches: ', decision_tree.branches)\n",
    "        # print('------------------------')\n",
    "        if decision_attr_val in decision_tree.branches:\n",
    "            return predict(decision_tree.branches[decision_attr_val], x)\n",
    "        else:\n",
    "            try:\n",
    "                return decision_tree.branches[not decision_tree].prediction\n",
    "            except:\n",
    "                return True\n",
    "\n",
    "        \n",
    "def accuracy(decision_tree, X, y_true):\n",
    "    '''Find the accuracy of given decision tree on dataset X with \n",
    "    ground truth y_true'''\n",
    "\n",
    "    # y_pred = np.apply_along_axis(lambda x: predict(decision_tree, x), 1, X)\n",
    "    y_pred = []\n",
    "    for x in X:\n",
    "        try:\n",
    "            y_pred.append(predict(decision_tree, x))\n",
    "        except:\n",
    "            y_pred.append(True)\n",
    "\n",
    "    return np.average(y_pred == y_true)\n",
    "\n",
    "def cross_validate(function_list, depth):\n",
    "    splits = {0, 1, 2, 3}\n",
    "    accuracies = []\n",
    "    for validation_idx in splits:\n",
    "        validation_data = read_dataset('../Dataset/Updated_CVSplits/updated_training0{}.txt'.format(validation_idx))\n",
    "        training_data = []\n",
    "    \n",
    "        for training_idx in splits - {validation_idx}:\n",
    "            training_data += read_dataset('../Dataset/Updated_CVSplits/updated_training0{}.txt'.format(training_idx))\n",
    "        \n",
    "        X_train = build_features(training_data, function_list)\n",
    "        y_train = np.array([d.label for d in training_data])\n",
    "        X_validate = build_features(validation_data, function_list)\n",
    "        y_validate = np.array([d.label for d in validation_data])\n",
    "        attributes = set(range(0, (X_train.shape[1])))        \n",
    "        \n",
    "        dec_tree = ID3(X_train, attributes, y_train, max_depth=depth)\n",
    "        accuracies.append(accuracy(dec_tree, X_validate, y_validate))\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "def q1():\n",
    "    training_data = read_dataset('../Dataset/updated_train.txt')\n",
    "    test_data = read_dataset('../Dataset/updated_test.txt')\n",
    "\n",
    "    X_train = build_features(training_data, func_list)\n",
    "    y_train = np.array([d.label for d in training_data])\n",
    "    attributes = set(range(0, (X_train.shape[1])))\n",
    "\n",
    "    dec_tree = ID3(X_train, attributes, y_train)\n",
    "\n",
    "    X_test = build_features(test_data, func_list)\n",
    "    y_test = np.array([d.label for d in test_data])\n",
    "    print(\"=====================================================\")\n",
    "    print(\"1(c) : Training accuracy = \", accuracy(dec_tree, X_train, y_train))\n",
    "    print(\"=====================================================\\n\")\n",
    "    print(\"======================= 1 (d) =======================\")\n",
    "    print(\"1(d) : Test accuracy     = \", accuracy(dec_tree, X_test, y_test))\n",
    "    print(\"=====================================================\\n\")\n",
    "    print(\"=====================================================\")\n",
    "    print(\"1 (e) Max depth          = \", len(func_list))\n",
    "    print(\"=====================================================\\n\")\n",
    "\n",
    "def q2(depths):\n",
    "    print(\" ___________________________________________________ \")\n",
    "    print(\"|              Cross Validation results             |\")\n",
    "    print(\"|---------------------------------------------------|\")\n",
    "    print(\"|{: ^16}|{: ^16}|{: ^17}|\".format(\"Depth\", \"Mean\", \"Std dev\"))\n",
    "    print(\"|---------------------------------------------------|\")\n",
    "\n",
    "    for depth in depths:\n",
    "        accuracies = cross_validate(func_list, depth)\n",
    "        mean, std_dev = np.mean(accuracies), np.std(accuracies)\n",
    "        print(\"|{: ^16}|{: ^16.5}|{: ^17.5}|\".format(depth, mean, std_dev))\n",
    "    print(\"|___________________________________________________|\")\n",
    "\n",
    "    training_data = read_dataset('../Dataset/updated_train.txt')\n",
    "    test_data = read_dataset('../Dataset/updated_test.txt')\n",
    "\n",
    "    X_train = build_features(training_data, func_list)\n",
    "    y_train = np.array([d.label for d in training_data])\n",
    "    attributes = set(range(0, (X_train.shape[1])))\n",
    "\n",
    "    dec_tree = ID3(X_train, attributes, y_train, max_depth=5)\n",
    "\n",
    "    X_test = build_features(test_data, func_list)\n",
    "    y_test = np.array([d.label for d in test_data])\n",
    "    print()\n",
    "    print(\"Using max_depth = 5 from the cross validation results\")\n",
    "    print(\"=====================================================\")\n",
    "    print(\"2(b) : Training accuracy = \", accuracy(dec_tree, X_train, y_train))\n",
    "    print(\"=====================================================\\n\")\n",
    "    print(\"======================= 1 (d) =======================\")\n",
    "    print(\"2(c) : Test accuracy     = \", accuracy(dec_tree, X_test, y_test))\n",
    "    print(\"=====================================================\\n\")\n",
    "\n",
    "\n",
    "func_list = [lambda x: len(x.split()[0]) > len(x.split()[-1]),     # length of first name more than last name\n",
    "             lambda x: len(x.split()) > 2,                         # do they have a middle name\n",
    "             lambda x: x.split()[0][0] == x.split()[-1][0],        # first letter of first and last name are same\n",
    "             lambda x: x.split()[0][0] < x.split()[-1][0],         # first letter is smaller than last name             \n",
    "             lambda x: x[1] in ['a','e','i','o','u'],              # second letter of first name is vowel\n",
    "             lambda x: len(x.split()[-1])%2 == 0,                  # number of letters in last name is even\n",
    "             lambda x: sum(map(ord, x))%2 == 0,\n",
    "             lambda x: x[2] in {'a', 'e', 'i', 'o', 'u'},\n",
    "             lambda x: x[3] in {'a', 'e', 'i', 'o', 'u'},\n",
    "             lambda x: x[4] in {'a', 'e', 'i', 'o', 'u'},\n",
    "             lambda x: x[5] in {'a', 'e', 'i', 'o', 'u'},\n",
    "             lambda x: len(set(x)) > 4,\n",
    "             lambda x: len(set(x)) > 8,\n",
    "             lambda x: len(list(filter(lambda x: x in {'a', 'e', 'i', 'o', 'u'}, x)))%2==0,\n",
    "             lambda x: 'a' < x[0] and x[0] <= 'k',\n",
    "             lambda x: 'k' < x[0] and x[0] <= 'r',\n",
    "             lambda x: 'r' < x[0] and x[0] <= 'z',\n",
    "             lambda x: 'a' < x[1] and x[1] <= 'k',\n",
    "             lambda x: 'k' < x[1] and x[1] <= 'r',\n",
    "             lambda x: 'r' < x[1] and x[1] <= 'z',\n",
    "             lambda x: 'a' < x[2] and x[2] <= 'k',\n",
    "             lambda x: 'k' < x[2] and x[2] <= 'r',\n",
    "             lambda x: 'r' < x[2] and x[2] <= 'z'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "1(c) : Training accuracy =  0.997752808989\n",
      "=====================================================\n",
      "\n",
      "======================= 1 (d) =======================\n",
      "1(d) : Test accuracy     =  0.918918918919\n",
      "=====================================================\n",
      "\n",
      "=====================================================\n",
      "1 (e) Max depth          =  23\n",
      "=====================================================\n",
      "\n",
      " ___________________________________________________ \n",
      "|              Cross Validation results             |\n",
      "|---------------------------------------------------|\n",
      "|     Depth      |      Mean      |     Std dev     |\n",
      "|---------------------------------------------------|\n",
      "|       1        |    0.69595     |     0.20419     |\n",
      "|       2        |    0.82432     |    0.018573     |\n",
      "|       3        |    0.87387     |    0.014244     |\n",
      "|       4        |    0.85811     |    0.013325     |\n",
      "|       5        |    0.89414     |    0.013325     |\n",
      "|       10       |    0.87838     |    0.026649     |\n",
      "|       15       |    0.87838     |    0.030217     |\n",
      "|       20       |    0.87838     |    0.030217     |\n",
      "|___________________________________________________|\n",
      "\n",
      "Using max_depth = 5 from the cross validation results\n",
      "=====================================================\n",
      "2(b) : Training accuracy =  0.930337078652\n",
      "=====================================================\n",
      "\n",
      "======================= 1 (d) =======================\n",
      "2(c) : Test accuracy     =  0.918918918919\n",
      "=====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1()\n",
    "q2([1, 2, 3, 4, 5, 10, 15, 20])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
