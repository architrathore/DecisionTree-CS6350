{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of decision tree for Machine Learning - CS6350\n",
    "Author: Archit Rathore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataRecord:\n",
    "    '''Class to store individual data points read from train and test files'''\n",
    "\n",
    "    def __init__(self, label, name):\n",
    "        self.label = 1 if label == '+' else 0\n",
    "        self.name = ' '.join(name).lower()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.label) + ' ' + self.name\n",
    "    \n",
    "class DecisionNode:\n",
    "    '''Represent a node in the decision tree'''\n",
    "    def __init__(self, attr_index):\n",
    "        self.attr_index = attr_index    # index of attribute that is tested in the \n",
    "                                        # feature array, -1 for leaf nodes\n",
    "        self.prediction = None          # the prediction from this node, None for all non-leaf nodes\n",
    "        self.branches = {}              # dictionary holding subtrees of the tree, empty for leaf nodes\n",
    "        \n",
    "    def __repr__(self):\n",
    "        if self.attr_index == -1:\n",
    "            return '[\"{0}\"]'.format(self.prediction)\n",
    "        else:\n",
    "            return '[{}]'.format(str(self.attr_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filepath):\n",
    "    '''Read train or test file and return a list DataRecord objects'''\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip().split()\n",
    "            label, name = line[0], line[1:]\n",
    "            data.append(DataRecord(label, name))\n",
    "    return data\n",
    "\n",
    "\n",
    "def build_features(dataset, func_list):\n",
    "    '''Apply a sequence of functions to the dataset to get a feature array\n",
    "    for each data point'''\n",
    "    n, d = len(dataset), len(func_list)\n",
    "    X = np.empty((n, d), dtype=int)\n",
    "    for idx, datapoint in enumerate(dataset):\n",
    "        X[idx] = np.array([f(datapoint.name) for f in func_list])\n",
    "    return X\n",
    "\n",
    "\n",
    "def check_same_label(arr):\n",
    "    '''Check if all elements in arr are identical and return it'''\n",
    "    return len(set(arr)) == 1\n",
    "\n",
    "\n",
    "def get_entropy(labels):\n",
    "    prob_pos, prob_neg = sum(labels == 1)/len(labels), sum(labels == 0)/len(labels)\n",
    "    if prob_pos == 0 or prob_neg == 0:\n",
    "        return 0\n",
    "    entropy = prob_pos * np.log(prob_pos) + prob_neg * np.log(prob_neg)\n",
    "    return -1.0 * entropy\n",
    "    \n",
    "    \n",
    "def find_best_split(S, attributes, labels):\n",
    "    # Return the first attribute for now\n",
    "    # return list(attributes)[0]\n",
    "    '''Implements information gain based on reduction in entropy\n",
    "    for feature selection'''\n",
    "    \n",
    "    if len(attributes) == 0:\n",
    "        return next(iter(attributes))\n",
    "    max_info_gain = 0\n",
    "    best_split_attribute = None\n",
    "    root_node_entropy = get_entropy(labels)\n",
    "    for attribute in attributes:\n",
    "        attribute_value_set = set(S[:, attribute])\n",
    "        weighted_entropy = 0\n",
    "        for value in attribute_value_set:\n",
    "            value_indices = S[:, attribute] == value\n",
    "            value_labels = labels[value_indices]\n",
    "            weighted_entropy += len(value_labels) * get_entropy(value_labels)\n",
    "        info_gain = root_node_entropy - weighted_entropy/len(S)\n",
    "        if info_gain >= max_info_gain:\n",
    "            best_split_attribute = attribute\n",
    "            max_info_gain = info_gain\n",
    "    return best_split_attribute\n",
    "\n",
    "\n",
    "def ID3(S, attributes, labels):\n",
    "    '''ID3 algorithm for building a decision tree'''\n",
    "\n",
    "    # If all examples have the same label\n",
    "    if check_same_label(labels) or len(attributes) == 0:\n",
    "        # Return a single node tree with the label\n",
    "        leaf_node = DecisionNode(-1)\n",
    "        leaf_node.prediction = labels[0]\n",
    "        return leaf_node\n",
    "\n",
    "    else:\n",
    "        # Create a root node for tree\n",
    "        root_node = DecisionNode(-1)\n",
    "\n",
    "        # print(\"Attributes\", attributes)\n",
    "        # Find attribute A that best classifies the dataset S\n",
    "        splitting_attr = find_best_split(S, attributes, labels)\n",
    "        root_node.attr_index = splitting_attr\n",
    "\n",
    "        # For each value v that the A can take:\n",
    "        for split_val in set(S[:, splitting_attr]):\n",
    "            # Find subset of examples S_v with A = v\n",
    "\n",
    "            # Get indices with splitting_att = split_val\n",
    "            indices = S[:, splitting_attr] == split_val\n",
    "            # indices is a boolean indicator array\n",
    "\n",
    "            # If S_v is empty\n",
    "            if sum(indices) == 0:\n",
    "                # Find the most common label in S\n",
    "                most_common_label = Counter(labels).most_common(1)\n",
    "                # Create a leaf node with this label\n",
    "                leaf_node = DecisionNode(-1)\n",
    "                leaf_node.prediction = most_common_label\n",
    "                # Add the leaf node to this branch of node\n",
    "                root_node.branches[split_val] = leaf_node\n",
    "\n",
    "            else:\n",
    "                # Add subtree ID3(S_v, Attributes - {A}, Label_v)\n",
    "                S_v = S[indices]\n",
    "                label_v = labels[indices]\n",
    "                remaining_attr = attributes - {splitting_attr}\n",
    "                root_node.branches[split_val] = ID3(S_v, remaining_attr, label_v)\n",
    "\n",
    "        return root_node\n",
    "\n",
    "\n",
    "def print_tree(decision_tree):\n",
    "    '''Do a level order traversal to print the decision tree'''\n",
    "    # Create two queues, one holds the current depth nodes\n",
    "    # and the other holds the nodes of the next level\n",
    "    curr_level, next_level = [], []\n",
    "    \n",
    "    # Append the root to current level's queue\n",
    "    curr_level.append(decision_tree)\n",
    "    \n",
    "    # While there are still nodes to visit\n",
    "    while (len(curr_level) > 0):\n",
    "        # Get the first node in queue\n",
    "        curr_node = curr_level.pop(0)\n",
    "        \n",
    "        # Print it's data\n",
    "        print(curr_node, end=' ')\n",
    "        \n",
    "        # For each child of the current node\n",
    "        for branch in curr_node.branches:\n",
    "            # Add all children to the next level's queue\n",
    "            next_level.append(curr_node.branches[branch])\n",
    "            \n",
    "        # If the current level's queue is empty\n",
    "        if len(curr_level) == 0:\n",
    "            print()\n",
    "            # Swap current and next level queues\n",
    "            curr_level, next_level = next_level, curr_level\n",
    "\n",
    "\n",
    "def predict(decision_tree, x):\n",
    "    '''Given a decision tree and input, find the prediction'''\n",
    "    \n",
    "    # If the given node is leaf\n",
    "    if decision_tree.attr_index == -1:\n",
    "        return decision_tree.prediction\n",
    "    \n",
    "    # Recursively call prediction based on current node's splitting attribute\n",
    "    else:\n",
    "        decision_attr_val = x[decision_tree.attr_index]\n",
    "        return predict(decision_tree.branches[decision_attr_val], x)\n",
    "    \n",
    "\n",
    "def accuracy(decision_tree, X, y_true):\n",
    "    '''Find the accuracy of given decision tree on dataset X with \n",
    "    ground truth y_true'''\n",
    "\n",
    "    y_pred = np.apply_along_axis(lambda x: predict(decision_tree, x), 1, X)\n",
    "    correct_pred_count = sum(y_pred == y_true)\n",
    "    return correct_pred_count/len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = read_dataset('../Dataset/updated_train.txt')\n",
    "\n",
    "func_list = [lambda x: x[0] in ['a','e','i','o','u'],    # first letter is vowel\n",
    "             lambda x: x[-1] in ['a','e','i','o','u'],   # last letter is vowel\n",
    "             lambda x: len(x)%2 == 0,                    # length of name is even\n",
    "             lambda x: len(x.split()) > 2                # do they have a middle name\n",
    "            ]\n",
    "\n",
    "X = build_features(dataset, func_list)\n",
    "y = np.array([d.label for d in dataset])\n",
    "attributes = set(range(0, (X.shape[1])))\n",
    "dec_tree = ID3(X, attributes, y)\n",
    "\n",
    "X_and = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "# dec_tree = ID3(X_and, {0,1}, y_and)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "xprime = build_features([DataRecord('+', 'bert bos'),\n",
    "                         DataRecord('-', 'alexander dewdney'),\n",
    "                         DataRecord('-', 'philip emeagwali')],\n",
    "                        func_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(dec_tree, xprime[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6179775280898876"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(dec_tree, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.70454545454545459"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\")\n",
    "X_train, y_train = X[:400], y[:400]\n",
    "X_test, y_test = X[401:], y[401:]\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "47px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
